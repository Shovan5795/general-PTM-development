{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def tokenize_and_extract_entity_indices(sentences, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    entity_start_positions = []\n",
    "    entity_end_positions = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Flatten the tensors to remove batch dimension added by `return_tensors`\n",
    "        input_ids.append(encoded['input_ids'].squeeze())\n",
    "        attention_masks.append(encoded['attention_mask'].squeeze())\n",
    "\n",
    "        # Find entity markers\n",
    "        start_index = (encoded['input_ids'].squeeze() == tokenizer.convert_tokens_to_ids('[E]')).nonzero(as_tuple=True)[0]\n",
    "        end_index = (encoded['input_ids'].squeeze() == tokenizer.convert_tokens_to_ids('[/E]')).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # In case of no entity marker found, assign 0 as a default index\n",
    "        entity_start_positions.append(start_index[0] if len(start_index) > 0 else torch.tensor(0))\n",
    "        entity_end_positions.append(end_index[0] if len(end_index) > 0 else torch.tensor(0))\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks), torch.stack(entity_start_positions), torch.stack(entity_end_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2003, 2)\n",
      "Validation shape: (223, 2)\n",
      "Test shape: (160, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = pd.read_csv(r'/home/shovan/Phd_Prelim/PTM/Datasets/Ubiq_training_data_Apr9.csv')\n",
    "dataset.head()\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.1, random_state=42, stratify=dataset['Label'])\n",
    "\n",
    "test_dataset = pd.read_csv(r'/home/shovan/Phd_Prelim/PTM/Datasets/ubiq_test_Apr9.csv')\n",
    "\n",
    "print(\"Train shape:\", train_dataset.shape)\n",
    "print(\"Validation shape:\", val_dataset.shape)\n",
    "print(\"Test shape:\", test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def noise_clean(sent):\n",
    "    if not sent.endswith('.'):\n",
    "        sent+='.'\n",
    "    return sent\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset[\"Sentence\"] = train_dataset[\"Sentence\"].apply(noise_clean)\n",
    "val_dataset[\"Sentence\"] = val_dataset[\"Sentence\"].apply(noise_clean)\n",
    "test_dataset[\"Sentence\"] = test_dataset[\"Sentence\"].apply(noise_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels Distribution:\n",
      "0    1237\n",
      "1     766\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Validation Labels Distribution:\n",
      "0    138\n",
      "1     85\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Test Labels Distribution:\n",
      "0    95\n",
      "1    65\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_labels_counts = train_dataset['Label'].value_counts()\n",
    "print(\"Training Labels Distribution:\")\n",
    "print(train_labels_counts)\n",
    "\n",
    "val_labels_counts = val_dataset['Label'].value_counts()\n",
    "print(\"\\nValidation Labels Distribution:\")\n",
    "print(val_labels_counts)\n",
    "\n",
    "test_labels_counts = test_dataset['Label'].value_counts()\n",
    "print(\"\\nTest Labels Distribution:\")\n",
    "print(test_labels_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2003 entries, 935 to 947\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Sentence   2003 non-null   object\n",
      " 1   Label      2003 non-null   int64 \n",
      " 2   num_words  2003 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 62.6+ KB\n",
      "58\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 223 entries, 134 to 1377\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Sentence   223 non-null    object\n",
      " 1   Label      223 non-null    int64 \n",
      " 2   num_words  223 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 7.0+ KB\n",
      "49\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160 entries, 0 to 159\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Sentence   160 non-null    object\n",
      " 1   Label      160 non-null    int64 \n",
      " 2   num_words  160 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.9+ KB\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "train_dataset[\"num_words\"] = train_dataset.Sentence.apply(lambda x: len(x.split()))\n",
    "train_dataset.info()\n",
    "print(train_dataset.num_words.max())\n",
    "\n",
    "val_dataset[\"num_words\"] = val_dataset.Sentence.apply(lambda x: len(x.split()))\n",
    "val_dataset.info()\n",
    "print(val_dataset.num_words.max())\n",
    "\n",
    "test_dataset[\"num_words\"] = test_dataset.Sentence.apply(lambda x: len(x.split()))\n",
    "test_dataset.info()\n",
    "print(test_dataset.num_words.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30524\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext'\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "# Free up memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.add_tokens(['[E]', '[/E]'])\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30524\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "bert.resize_token_embeddings(len(tokenizer))\n",
    "print(bert.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call for the train dataset\n",
    "train_input_ids, train_attention_masks, train_start_positions, train_end_positions = tokenize_and_extract_entity_indices(train_dataset['Sentence'].tolist(), tokenizer)\n",
    "\n",
    "# Similarly for validation and test datasets\n",
    "val_input_ids, val_attention_masks, val_start_positions, val_end_positions = tokenize_and_extract_entity_indices(val_dataset['Sentence'].tolist(), tokenizer)\n",
    "test_input_ids, test_attention_masks, test_start_positions, test_end_positions = tokenize_and_extract_entity_indices(test_dataset['Sentence'].tolist(), tokenizer)\n",
    "\n",
    "# Now, define a custom dataset\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, input_ids, masks, start_positions, end_positions, labels=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.masks = masks\n",
    "        self.start_positions = start_positions\n",
    "        self.end_positions = end_positions\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.masks[idx],\n",
    "            'start_position': self.start_positions[idx],\n",
    "            'end_position': self.end_positions[idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_labels = torch.tensor(train_dataset['Label'].tolist())\n",
    "val_labels = torch.tensor(val_dataset['Label'].tolist())\n",
    "test_labels = torch.tensor(test_dataset['Label'].tolist())\n",
    "\n",
    "train_data = ProteinDataset(train_input_ids, train_attention_masks, train_start_positions, train_end_positions, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data = ProteinDataset(val_input_ids, val_attention_masks, val_start_positions, val_end_positions, val_labels)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_data = ProteinDataset(test_input_ids, test_attention_masks, test_start_positions, test_end_positions, test_labels)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        # Updated to handle three times the embedding size due to concatenation of CLS and two entity token embeddings\n",
    "        self.fc1 = nn.Linear(768 * 3, 256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask, start_pos, end_pos):\n",
    "        # Getting outputs from BERT\n",
    "        outputs = self.bert(sent_id, attention_mask=mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # Extract the CLS token's embeddings\n",
    "        cls_hs = sequence_output[:, 0, :]  # CLS token is the first token\n",
    "\n",
    "        # Extract embeddings for start and end entity positions\n",
    "        start_embeddings = sequence_output[torch.arange(sequence_output.size(0)), start_pos]\n",
    "        end_embeddings = sequence_output[torch.arange(sequence_output.size(0)), end_pos]\n",
    "\n",
    "        # Concatenate CLS and entity embeddings\n",
    "        concatenated_embeddings = torch.cat((cls_hs, start_embeddings, end_embeddings), dim=1)\n",
    "\n",
    "        # Passing through the classifier layers\n",
    "        x = self.fc1(concatenated_embeddings)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.80962005 1.30744125]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "model = BERT_Arch(bert)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 2e-5, weight_decay = 0.001)\n",
    "#from transformers import AdamW\n",
    "#optimizer = AdamW(model.parameters(), lr = 1e-5)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes = np.unique(train_dataset['Label']), y=train_dataset['Label'])\n",
    "\n",
    "print(\"Class Weights:\",class_weights)\n",
    "\n",
    "# converting list of class weights to a tensor\n",
    "from cProfile import label\n",
    "\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to CPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "#cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "cross_entropy  = nn.CrossEntropyLoss(weight=weights)\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('Batch {:>5,} of {:>5,}.'.format(step, len(train_loader)))\n",
    "\n",
    "        # Unpack the batch directly into the respective tensors and move them to the device\n",
    "        sent_id = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_position'].to(device)\n",
    "        end_pos = batch['end_position'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        ypred = model(sent_id, mask, start_pos, end_pos)\n",
    "        loss = cross_entropy(ypred, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        ypred = ypred.detach().cpu().numpy()\n",
    "        total_preds.append(ypred)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,} of {:>5,}.'.format(step, len(dataloader)))\n",
    "\n",
    "        # Unpack the batch directly into the respective tensors and move them to the device\n",
    "        sent_id = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_position'].to(device)\n",
    "        end_pos = batch['end_position'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask, start_pos, end_pos)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "Batch    50 of   126.\n",
      "Batch   100 of   126.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m train_loss, _ \u001b[38;5;241m=\u001b[39m train()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#evaluate model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m valid_loss, _ \u001b[38;5;241m=\u001b[39m evaluate(val_loader)\n\u001b[1;32m     19\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(valid_loss)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#save the best model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [38], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  Batch \u001b[39m\u001b[38;5;132;01m{:>5,}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{:>5,}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(step, \u001b[38;5;28mlen\u001b[39m(dataloader)))\n\u001b[0;32m---> 10\u001b[0m batch \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     11\u001b[0m sent_id, mask, start_pos, end_pos, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn [38], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  Batch \u001b[39m\u001b[38;5;132;01m{:>5,}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{:>5,}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(step, \u001b[38;5;28mlen\u001b[39m(dataloader)))\n\u001b[0;32m---> 10\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     11\u001b[0m sent_id, mask, start_pos, end_pos, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate(val_loader)\n",
    "    \n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '/home/shovan/Phd_Prelim/PTM/Weights/ep10_ubiq_entity_May10.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "Test Accuracy: 0.9203\n",
      "Precision: 0.9267\n",
      "Recall: 0.9145\n",
      "F1: 0.9205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('/home/shovan/Phd_Prelim/PTM/Weights/ep10_ubiq_sumo_nedd_com_apr26.pt'))\n",
    "\n",
    "test_loss, test_preds = evaluate(test_loader)  # Pass the correct dataloader here\n",
    "\n",
    "# Convert predictions to label indices\n",
    "pred_labels = np.argmax(test_preds, axis=1)\n",
    "\n",
    "# Compute metrics\n",
    "test_accuracy = accuracy_score(test_dataset['Label'].to_numpy(), pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_dataset['Label'].to_numpy(), pred_labels, average='binary')\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: We find that all three TRANSFERASES can direct the MODIFICATION of TFIIEbetaand TFIIF, and we identify a [E]PROTEIN[/E] of MODIFICATION in TFIIEbeta.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: Increased affinity of [E]PROTEIN[/E] for CREB-binding protein (CBP) after CBP-induced MODIFICATION.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: The results of transcriptional activation assays with [E]PROTEIN[/E] MODIFICATION site mutants suggested that MODIFICATION of c-Myb at each of these five sites synergistically enhances c-Myb activity.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: These results uncover novel post-translational modifications of Max and suggest the potential regulation of specific [E]PROTEIN[/E] complexes by p300 and reversible MODIFICATION.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: Here we demonstrate that the C/H2 domain of CBP, which is critical for the TRANSFERASE activity, also directly interacts with the [E]PROTEIN[/E] of c-Myb.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: Here we demonstrate that the C/H2 domain of CBP, which is critical for the TRANSFERASE activity, also directly interacts with the negative regulatory domain (NRD) of [E]PROTEIN[/E].\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: The X-ray crystal structures of yeast Esa1 (yEsa1/KAT5) bound to a bisubstrate H4K16CoA inhibitor and [E]PROTEIN[/E] reveal that they are autoMODIFIED at a strictly conserved lysine residue in MYST proteins (yEsa1-K262 and hMOF-K274) in the enzyme active site.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: The X-ray crystal structures of yeast Esa1 (yEsa1/KAT5) bound to a [E]PROTEIN[/E] and human MOF (hMOF/KAT8/MYST1) reveal that they are autoMODIFIED at a strictly conserved lysine residue in MYST proteins (yEsa1-K262 and hMOF-K274) in the enzyme active site.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: p300 is then recruited to the gene promoter through the interaction with deMODIFIED [E]PROTEIN[/E] to MODIFY histone 3, leading to the enhancement of the expression of 12(S)-lipoxygenase.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: [E]PROTEIN[/E] recruits TRANSFERASES to modify chromatin and is, itself, MODIFIED in mammalian cells by several of these TRANSFERASES including p300/CBP, GCN5, and Tip60.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: p300 is then recruited to the gene promoter through the interaction with deMODIFIED Sp1 to MODIFY histone 3, leading to the enhancement of the expression of [E]PROTEIN[/E].\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: Prolonged stimulation of the cells with GROUP (9 h), however, caused the dissociation of histone deacetylase 1 (HDAC1) and the deMODIFICATION of Sp1, with the latter being able to recruit p300 that in turn caused the MODIFICATION and dissociation of histone 3, thus enhancing the expression of [E]PROTEIN[/E].\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: We also show that Lys4 MODIFICATION plays a prominent role in the p300-dependent activation of [E]PROTEIN[/E].\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: These findings provide an avenue for the autoposttranslational regulation of [E]PROTEIN[/E] proteins that is distinct from other TRANSFERASES but draws similarities to the phosphoregulation of protein kinases.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: The X-ray crystal structures of yeast [E]PROTEIN[/E] bound to a bisubstrate H4K16CoA inhibitor and human MOF (hMOF/KAT8/MYST1) reveal that they are autoMODIFIED at a strictly conserved lysine residue in MYST proteins (yEsa1-K262 and hMOF-K274) in the enzyme active site.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: Myc recruits TRANSFERASES to modify [E]PROTEIN[/E] and is, itself, MODIFIED in mammalian cells by several of these TRANSFERASES including p300/CBP, GCN5, and Tip60.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: The mutant [E]PROTEIN[/E] expressed in mouse B82 cells displayed normal EGF binding and in vivo autoMODIFICATION and was fully active in biological signal transduction as measured by EGF -stimulated gene transcription .\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: AB - [E]PROTEIN[/E] is a member of the ERM family , a family of cross-linkers between the plasma membrane and the actin cytoskeleton , which are known to be activated by MODIFICATION .\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: TI - Mutational removal of the [E]PROTEIN[/E] and Ser671 MODIFICATION sites alters substrate specificity and ligand -induced internalization of the epidermal growth factor receptor .\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: Because the biological responsiveness of the EGF receptor is regulated by MODIFICATION at several of these sites , we studied the functional consequences of removal of the [E]PROTEIN[/E] and Ser671 MODIFICATION sites using site -directed mutagenesis .\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: The inhibition of protein TRANSFERASE A ( PKA ) , known as an upstream TRANSFERASE of [E]PROTEIN[/E] , abolished Rac1 activation and moesin MODIFICATION by KCl .\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: Finally , the MODIFICATION pattern of Cos2 produced by baculovirus coinfection with wild-type Fu is almost identical to that of [E]PROTEIN[/E] isolated from S2 cells stimulated by Hh , indicating that MODIFICATION of serines 572 and 931 is a genuine Hh signaling event .\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: Recent studies on the role of conserved PPPSPxS motifs in the cytoplasmic tail of low-density lipoprotein receptor -related protein ( LRP , isoforms 5 and 6 ) culminated in a biochemical model : Wnt induces the MODIFICATION of LRP6 PPPSPxS motifs , which consequently access the catalytic pocket of GSK3 as pseudo-substrates , thus directly blocking its activity against [E]PROTEIN[/E] .\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: Wnt blocks this MODIFICATION event , thereby allowing [E]PROTEIN[/E] to accumulate and to co-activate transcription in the nucleus .\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: Recent studies on the role of conserved PPPSPxS motifs in the cytoplasmic tail of [E]PROTEIN[/E] culminated in a biochemical model : Wnt induces the MODIFICATION of LRP6 PPPSPxS motifs , which consequently access the catalytic pocket of GSK3 as pseudo-substrates , thus directly blocking its activity against beta-catenin .\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: In contrast, N629 is not used for MODIFICATION, but mutation of this site ([E]PROTEIN[/E]) causes a protein trafficking defect, which results in its intracellular retention.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: In summary, our data indicate that utilization of several MODIFICATION sites is important for [E]PROTEIN[/E] activity, whereas the structure of the attached GROUPS is not critical.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: Pulse-chase experiments show that the turnover rate of nonMODIFIED HERG channel is faster than that of the MODIFIED form, suggesting that MODIFICATION plays an important role in [E]PROTEIN[/E] channel stability.\n",
      "Predicted Label: 0, True Label: 1\n",
      "\n",
      "Text: Although plants contain substantial amounts of [E]PROTEIN[/E], the enzymes responsible for AGP MODIFICATION are largely unknown.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: We found differences between [E]PROTEIN[/E] GROUPS from patient sera compared to controls.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: Loss of MODIFICATION at N65 resulted in aggregation of [E]PROTEIN[/E], suggesting that MODIFICATION at this site is essential for proper folding of the enzyme.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: In conclusion, protein MODIFICATION of surface proteins may enhance C.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n",
      "Text: AtGALT2 specifically catalyzed incorporation of GROUP from GROUP to Hyp of model substrate acceptors having AGP peptide sequences, consisting of non-contiguous Hyp residues, such as (Ala-Hyp) repetitive units exemplified by chemically synthesized [E]PROTEIN[/E] and anhydrous hydrogen fluoride-deglycosylated d(AO)51.\n",
      "Predicted Label: 1, True Label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `dataset_tst['Sentence']` is your texts and `dataset_tst['Label']` are your true labels\n",
    "true_labels = test_dataset['Label'].to_numpy()\n",
    "predicted_labels = pred_labels  # This is obtained from your code\n",
    "\n",
    "# Find the indices where predicted and true labels differ\n",
    "mismatches = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "# Extract the texts and their predicted and true labels for the mismatches\n",
    "mismatched_texts = test_dataset['Sentence'].iloc[mismatches].values\n",
    "mismatched_predictions = predicted_labels[mismatches]\n",
    "mismatched_true_labels = true_labels[mismatches]\n",
    "\n",
    "# Display the results or process them further as needed\n",
    "for i, text in enumerate(mismatched_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Label: {mismatched_predictions[i]}, True Label: {mismatched_true_labels[i]}\\n\")\n",
    "\n",
    "# Depending on your dataset size and the number of mismatches, you might want to limit the number of outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# True labels\n",
    "true_labels = test_dataset['Label'].to_numpy()\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels = pred_labels\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences(sentences):\n",
    "    # Tokenize the sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move tensors to the correct device\n",
    "    input_ids = encoded_input['input_ids'].to(device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict without using named arguments\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "    \n",
    "    logits = outputs\n",
    "    # Add a dimension to logits if it's missing the batch dimension\n",
    "    if logits.dim() == 1:\n",
    "        logits = logits.unsqueeze(0)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [1]\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\n",
    "    \"Moreover, [E]PROTEIN[/E] is neddylated in vivo, and ATGP-GROUP conjugates accumulate in cells lacking either Ubp3p or its cofactor, Bre5p.\"\n",
    "]\n",
    "\n",
    "predicted_classes = predict_sentences(new_sentences)\n",
    "print(\"Predicted classes:\", predicted_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
